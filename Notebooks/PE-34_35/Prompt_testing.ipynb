{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf7aa95",
   "metadata": {},
   "source": [
    "**Libraries Required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0faa405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pinecone\n",
    "# Importing necessary modules from langchain package\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.pinecone import Pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector, MaxMarginalRelevanceExampleSelector\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory,ConversationEntityMemory\n",
    "from langchain.chains import LLMChain\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e49ecc4-c9df-4577-9b5b-fb0bba53acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] =  \"\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]= \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]= \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"\"\n",
    "\n",
    "# QueryProcessor class definition\n",
    "class QueryProcessor:\n",
    "    def __init__(self, prompt_file_path):\n",
    "        # Setting the prompt file path\n",
    "        self.prompt_file_path = \"....\"\n",
    "        self.llm = ChatOpenAI(model='gpt-4', temperature=0.0)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.output_parser = StrOutputParser()\n",
    "        self.memory = ConversationEntityMemory(llm=self.llm)\n",
    "        self.example_prompt = PromptTemplate(\n",
    "                input_variables=[\"output\"],\n",
    "                template=\"Format: {output}\",\n",
    "            )\n",
    "        # Loading the example selector\n",
    "        self.example_selector = self.load_example_selector()\n",
    "\n",
    "    # Function to load example selector\n",
    "    def load_example_selector(self):\n",
    "        # Reading examples from file\n",
    "        examples = self.read_examples_from_file(self.prompt_file_path)\n",
    "        # Creating MaxMarginalRelevanceExampleSelector instance from examples\n",
    "        return MaxMarginalRelevanceExampleSelector.from_examples(examples, self.embeddings, FAISS, k=1)\n",
    "\n",
    "    # Function to read examples from file\n",
    "    def read_examples_from_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            examples = json.load(file)\n",
    "        return examples\n",
    "\n",
    "    # Function to process query\n",
    "    def process_query(self, response, query, namespace, chat_history):\n",
    "        # Creating Pinecone vectorstore\n",
    "        vectorstore = Pinecone.from_existing_index(index_name='testing', embedding=self.embeddings, namespace=namespace)\n",
    "        # Creating retriever\n",
    "        retriever = vectorstore.as_retriever(search_type=\"mmr\",\n",
    "                                             search_kwargs={\"k\": 8, 'fetch_k': 2, \"score_threshold\": 0.9})\n",
    "\n",
    "        # Creating OpenAI client\n",
    "        client = OpenAI()\n",
    "\n",
    "        # Function to extract entity from query and response\n",
    "        def extract_entity(query, response):\n",
    "            query_words_to_check = [\"this\", \"that\", \"them\", \"above\"]\n",
    "            pattern = r'\\b(?:' + '|'.join(map(re.escape, query_words_to_check)) + r')\\b'\n",
    "\n",
    "            if re.search(pattern, query, flags=re.IGNORECASE):\n",
    "                if response:\n",
    "                    client = OpenAI()\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=\"gpt-4\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are an intelligent chatbot. Only extract wine names from the given response. If there is no wine name in the response, return nothing.\"},\n",
    "                            {\"role\": \"user\", \"content\": response}\n",
    "                        ]\n",
    "                    )\n",
    "                    entity = completion.choices[0].message.content\n",
    "                    return entity\n",
    "                else:\n",
    "                    entity = ''\n",
    "                    return entity\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Extracting entity\n",
    "        entity = extract_entity(query, response)\n",
    "        data_fetched_li = []\n",
    "        # Getting relevant documents from retriever\n",
    "        data_fetched = retriever.get_relevant_documents(query + \" \" + str(entity))\n",
    "        # Storing retrieved data in a list\n",
    "        for i in data_fetched:\n",
    "            data_fetched_li.append(i.page_content)\n",
    "\n",
    "        # Combining retrieved data\n",
    "        db_result = \"\".join(data_fetched_li)\n",
    "        # Constructing prompt\n",
    "        prompt_sel = self.construct_prompt(query).split(\"\\n\\n\")[0]\n",
    "        # Generating response\n",
    "        llm_result = self.generate_response(query, prompt_sel, db_result)\n",
    "\n",
    "        # Extracting response from result\n",
    "        response_dict = llm_result['text'].split(\":\")[-1].strip()\n",
    "        # Appending query-response pair to chat history\n",
    "        chat_history.append({\"Human:\": query, \"AI:\": response_dict})\n",
    "        return response_dict\n",
    "\n",
    "    # Function to construct prompt\n",
    "    def construct_prompt(self, query):\n",
    "        dynamic_prompt = FewShotPromptTemplate(\n",
    "            example_selector=self.example_selector,\n",
    "            example_prompt=self.example_prompt,\n",
    "            suffix=\"User: {user_query} \",\n",
    "            input_variables=[\"user_query\"],\n",
    "        )\n",
    "\n",
    "        dynamic_prompt_str = str(dynamic_prompt.format(user_query=query)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "        return dynamic_prompt_str\n",
    "\n",
    "    # Function to generate response based on prompt\n",
    "    def generate_response(self,query, prompt_sel, db_result):\n",
    "        context_prompt = PromptTemplate(\n",
    "            input_variables=[\"example\", \"context\"],\n",
    "            template=\"\"\"Your job is to provide the relevant response.\n",
    "                        Given the example below, follow the format of example and use context to provide answer.\n",
    "                        <example>\n",
    "                        {example}\n",
    "                        </example>\n",
    "                        \n",
    "                        You are provide a context, based on which you have to generate response:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "                        If someone greets with \"hi\" or \"hello,\" always greet back.\n",
    "                        If someone asks \"How are you?\" always respond with \"I am good. I am a digital sommelier here to help you select the best drink. Can I help you select something?\n",
    "                        Your answer should be short, concise, and meaningful with relevance to above only.\n",
    "                        If the context is valid, the model should utilize the \n",
    "                        provided example to structure its response. The example line serves as a guideline/format \n",
    "                        for the chatbot's response, ensuring consistency and coherence in its interactions.\"\"\")\n",
    "\n",
    "        example_selected_prompt = context_prompt.format(example=prompt_sel, context=db_result)\n",
    "\n",
    "        final_prompt = PromptTemplate(\n",
    "            template=str(example_selected_prompt) +\n",
    "                     \"\"\"You are provided with information about the chat with the Human, if relevant.\n",
    "                     User:{input}\n",
    "                     Response:\"\"\",\n",
    "            input_variables=['input'])\n",
    "\n",
    "        # Creating LLMChain \n",
    "        llm_response_chain = LLMChain(llm=self.llm, prompt=final_prompt, memory=self.memory)\n",
    "        return llm_response_chain({'input': query})\n",
    "\n",
    "    # Function to select workflow\n",
    "    def select_workflow(self, query):\n",
    "        workflow = (PromptTemplate.from_template(\n",
    "            \"\"\"Given the user question below, classify it as either being about `Drinks, wine, beer, food`, `Healthcare, medicines`, or `Other`.\n",
    "\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "\n",
    "            Classification:\"\"\"\n",
    "                )\n",
    "                | self.llm\n",
    "                | self.output_parser)\n",
    "\n",
    "        # Invoking workflow and returning classification\n",
    "        classification = workflow.invoke({\"question\": query})\n",
    "        return classification\n",
    "\n",
    "# Initializing QueryProcessor with prompt file path\n",
    "prompt_file_path = \"......\"\n",
    "query_processor = QueryProcessor(prompt_file_path)\n",
    "\n",
    "chatgpt_response = ''\n",
    "selected_workflow = None\n",
    "namespace=None\n",
    "chat_history = []\n",
    "\n",
    "# querying\n",
    "while True:\n",
    "    query = str(input())\n",
    "\n",
    "    if selected_workflow == None:\n",
    "        classification = query_processor.select_workflow(query)\n",
    "        print(classification)\n",
    "        if \"healthcare\" in classification.lower():\n",
    "            namespace = \"cvs_health\"\n",
    "            selected_workflow = namespace\n",
    "        elif 'drinks' in classification.lower():\n",
    "            namespace = 'wine'\n",
    "            selected_workflow = namespace\n",
    "        else:\n",
    "            namespace = None\n",
    "    response = query_processor.process_query(response= chatgpt_response, query = query, namespace=namespace, chat_history=chat_history)\n",
    "    chatgpt_response = response\n",
    "    print(\"chatgpt_response\", chatgpt_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dba541-a67d-471d-a3e4-526347611cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427caa57-0701-4b32-9e00-73d2f314aef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c1467-8673-4484-ab2e-b320263c8d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc27ef8-cfd6-413f-ae9e-64031fd0d0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
